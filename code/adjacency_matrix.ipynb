{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to generate adjacency matrices of our scripts in the juliet dataset to be used as input for our neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import networkx as nx\n",
    "from preprocess_code import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/buffer_overflow_data.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/adj.pickle\",'rb') as f:\n",
    "    adj = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data.copy()\n",
    "del labels['Unnamed: 0']\n",
    "del labels['Unnamed: 0.1']\n",
    "del labels['filename']\n",
    "del labels['code']\n",
    "del labels['flaw']\n",
    "del labels['flaw_loc']\n",
    "labels = labels.drop_duplicates().sort_values('testcase_ID').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = adj.rename(columns={0: 'testcase_ID', 1: 'matrix'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_df = pd.merge(labels, adj, on='testcase_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_df = adj_df[['testcase_ID', 'matrix', 'bug']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_df['matrix_size'] = adj_df.matrix.apply(lambda x: x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_size = 614\n",
    "adj_df = adj_df[adj_df['matrix_size'] <= matrix_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = data[data.set_index(['testcase_ID']).index.isin(adj_df.set_index(['testcase_ID']).index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1248)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_edge_list1(testcase, **kwargs):\n",
    "    \"\"\"\n",
    "    Takes in a list of files/datapoints from juliet.csv.zip \n",
    "    or (as loaded with pandas) matching one particular testcase, \n",
    "    and returns an edge list of its graph representation.\n",
    "    \"\"\"\n",
    "    parse_list = [\n",
    "        (datapoint.filename, datapoint.code)\n",
    "        for datapoint in testcase.itertuples()\n",
    "    ]\n",
    "\n",
    "    primary = find_primary_source_file(testcase)\n",
    "\n",
    "    # Parse the source code with clang, and get out an ast:\n",
    "    index = clang.cindex.Index.create()\n",
    "    translation_unit = index.parse(\n",
    "        path=primary.filename,\n",
    "        unsaved_files=parse_list,\n",
    "    )\n",
    "    ast_root = translation_unit.cursor\n",
    "\n",
    "    # Memoise/concretise the ast so that we can consistently\n",
    "    # modify it, then number each node in the tree uniquely.\n",
    "    concretise_ast(ast_root)\n",
    "    number_ast_nodes(ast_root)\n",
    "\n",
    "    # Next, construct an edge list for the graph2vec input:\n",
    "    edgelist = generate_edgelist(ast_root)\n",
    "    \n",
    "    edgelist_representation = {\n",
    "        \"edges\": edgelist,\n",
    "    }\n",
    "\n",
    "    # Explicitly delete clang objects\n",
    "    del translation_unit\n",
    "    del ast_root\n",
    "    del index\n",
    "\n",
    "    return json.dumps(edgelist_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dask_data = dd.from_pandas(data, npartitions=20)\n",
    "\n",
    "# generate the graphs for all the testcases in the dataset \n",
    "\n",
    "graphs = data.groupby(['testcase_ID']).apply(\n",
    "        generate_edge_list1,\n",
    "        axis='columns',\n",
    "        meta=('generate_edge_list', 'unicode'),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_adj_matrix1(testcase):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes in a list of files/datapoints from buffer_overflow_data.csv.gz \n",
    "    matching one particular testcase, and generates an adjacency matrix \n",
    "    from the edgelist created.\n",
    "    \"\"\"\n",
    "    \n",
    "    # extracting the list of edges \n",
    "\n",
    "    x = testcase.split('edges\": ')\n",
    "    x = x[1].split('}')\n",
    "    x = ast.literal_eval(x[0])\n",
    "    \n",
    "#     return x\n",
    "\n",
    "    # generating the matrix\n",
    "    \n",
    "    G = nx.Graph()\n",
    "\n",
    "    G.add_edges_from(x)\n",
    "\n",
    "    A = nx.adjacency_matrix(G)\n",
    "\n",
    "    B = A.todense()\n",
    "\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe containing the testcase ID and its adjacency matrix \n",
    "adjacency_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_df['testcase_ID'] = data.testcase_ID.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graphs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-5cb3589be10b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# adj_matrices = graphs.apply(gen_adj_matrix1, meta = ('generate_adj_matrices', 'O'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0madj_matrices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_adj_matrix1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'graphs' is not defined"
     ]
    }
   ],
   "source": [
    "# kernel dies when there are more than 200 datapoints\n",
    "\n",
    "# adj_matrices = graphs.apply(gen_adj_matrix1, meta = ('generate_adj_matrices', 'O'))\n",
    "adj_matrices = graphs.apply(gen_adj_matrix1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adj_matrices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-95c2d827c74c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# adj_matrices = pd.DataFrame(adj_matrices)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0madj_matrices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madj_matrices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'adj_matrices' is not defined"
     ]
    }
   ],
   "source": [
    "# adj_matrices = pd.DataFrame(adj_matrices)\n",
    "adj_matrices = adj_matrices.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adj_matrices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e435eabdde2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# adj_matrices = adj_matrices.compute()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0madj_matrices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madj_matrices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'testcase_ID'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'adj_matrices' is not defined"
     ]
    }
   ],
   "source": [
    "## TODO: in a DASK framework reset_index is not a recognized function like pandas, fix this bug\n",
    "\n",
    "# adj_matrices = adj_matrices.compute()\n",
    "adj_matrices = adj_matrices.reset_index(level='testcase_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adjacency_df['adj_matrix'] = adj_matrices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_df = adjacency_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_df.to_csv(\"../data/adj_df.csv.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concretise_ast(node):\n",
    "    \"\"\"\n",
    "    Everytime you run .get_children() on a clang ast node, it\n",
    "    gives you new objects. So if you want to modify those objects\n",
    "    they will lose their changes everytime you walk the tree again.\n",
    "    To avoid this problem, concretise_ast walks the tree once,\n",
    "    saving the resulting list from .get_children() into a a concrete\n",
    "    list inside the .children.\n",
    "    You can then use .children to consistently walk over tree, and\n",
    "    it will give you the same objects each time.\n",
    "    \"\"\"\n",
    "    node.children = list(node.get_children())\n",
    "\n",
    "    for child in node.children:\n",
    "        counter = concretise_ast(child)\n",
    "\n",
    "def number_ast_nodes(node, counter=1):\n",
    "    \"\"\"\n",
    "    Given a concretised clang ast, assign each node with a unique\n",
    "    numerical identifier. This will be accessible via the .identifier\n",
    "    attribute of each node.\n",
    "    \"\"\"\n",
    "    node.identifier = counter\n",
    "    counter += 1\n",
    "\n",
    "    node.children = list(node.get_children())\n",
    "    for child in node.children:\n",
    "        counter = number_ast_nodes(child, counter)\n",
    "\n",
    "    return counter\n",
    "\n",
    "\n",
    "def generate_ast_roots(testcase, **kwargs):\n",
    "    \"\"\"\n",
    "    Takes in a list of files/datapoints from juliet.csv.zip (as loaded with pandas) matching one particular\n",
    "    testcase, and preprocesses it ready for the feature matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    parse_list = [\n",
    "        (datapoint.filename, datapoint.code)\n",
    "        for datapoint in testcase.itertuples()\n",
    "    ]\n",
    "\n",
    "    primary = find_primary_source_file(testcase)\n",
    "\n",
    "    # Parse the source code with clang, and get out an ast:\n",
    "    index = clang.cindex.Index.create()\n",
    "    translation_unit = index.parse(\n",
    "        path=primary.filename,\n",
    "        unsaved_files=parse_list,\n",
    "    )\n",
    "    ast_root = translation_unit.cursor\n",
    "    \n",
    "    concretise_ast(ast_root)\n",
    "    number_ast_nodes(ast_root)\n",
    "    \n",
    "    return ast_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sam's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ast_roots = dat.groupby(['testcase_ID']).apply(generate_ast_roots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try some other code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ast_roots = dat.groupby(['testcase_ID']).apply(generate_ast_roots,axis='columns',\n",
    "        meta=('generate_ast_roots', 'unicode'),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "Error in atexit._run_exitfuncs:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/shutil.py\", line 501, in rmtree\n",
      "    onerror(os.path.islink, path, sys.exc_info())\n",
      "  File \"/usr/lib/python3.7/shutil.py\", line 499, in rmtree\n",
      "    raise OSError(\"Cannot call rmtree on a symbolic link\")\n",
      "OSError: Cannot call rmtree on a symbolic link\n"
     ]
    }
   ],
   "source": [
    "for index, row in ast_roots.iteritems():\n",
    "    import pdb;pdb.set_trace()\n",
    "    print(row)\n",
    "        # do matrix append maybe if kernel dies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# example_node = ast_roots.iloc[0].children[19]\n",
    "# dir(example_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the columns for the feature matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_colnames(ast_root):\n",
    "    \"\"\"\n",
    "    Given a concretised & numbered clang ast, returns a set of node kinds to be used as columns in feature matrix\n",
    "    \"\"\"\n",
    "    features =  set()\n",
    "\n",
    "\n",
    "    def walk_tree_and_set_features(node):\n",
    "        out_degree = len(node.children)\n",
    "        in_degree = 1\n",
    "        degree = out_degree + in_degree\n",
    "        \n",
    "        features.add(str(node.kind))\n",
    "\n",
    "\n",
    "        for child in node.children:\n",
    "            walk_tree_and_set_features(child)\n",
    "\n",
    "    walk_tree_and_set_features(ast_root)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def generate_spelling(ast_root):\n",
    "    \"\"\"\n",
    "    Given a concretised & numbered clang ast, returns a set of node spellings to be used later\n",
    "    in constructing the columns in feature matrix\n",
    "    \"\"\"\n",
    "    spelling =  set()\n",
    "\n",
    "\n",
    "    def walk_tree_and_set_features(node):\n",
    "        out_degree = len(node.children)\n",
    "        in_degree = 1\n",
    "        degree = out_degree + in_degree\n",
    "        \n",
    "        spelling.add(node.spelling)\n",
    "\n",
    "        for child in node.children:\n",
    "            walk_tree_and_set_features(child)\n",
    "\n",
    "    walk_tree_and_set_features(ast_root)\n",
    "\n",
    "    return spelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating unique set of node kinds and node spellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = ast_roots.apply(generate_colnames)\n",
    "spelling = ast_roots.apply(generate_spelling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining a set of all final columns in feature matrix to be one-hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_colnames = set()\n",
    "final_colnames.update(['Identifier', 'WriteToPointer', 'SizeOf', 'Alloc'])\n",
    "for i in range(len(colnames)):\n",
    "    final_colnames.update(colnames.iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set of all node spellings to fish out the important ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_spelling = set()\n",
    "for i in range(len(spelling)):\n",
    "    final_spelling.update(spelling.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_colnames = pd.Series(list(final_colnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(final_colnames)):\n",
    "#     final_colnames.iloc[i] = 'kind_' + final_colnames.iloc[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually pick out important node spellings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [feature for feature in final_spelling if 'Alloc' in feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alloc_list = ['__builtin_alloca', \n",
    "              '__alloc', \n",
    "              'malloc', \n",
    "              'valloc', \n",
    "              '__alloc_on_copy', \n",
    "              '__alloc_on_move', \n",
    "              'calloc', \n",
    "              'realloc', \n",
    "              'alloca',\n",
    "              'ALLOCA'\n",
    "             ]\n",
    "\n",
    "sizeOf_list = ['std::aligned_storage<sizeof(_Tp), __alignof(_Tp)>'\n",
    "              ]\n",
    "\n",
    "writeToPointer_list = ['__builtin_memmove', \n",
    "                       '__builtin_memcpy', \n",
    "                       'wmempcpy', \n",
    "                       'wmemmove'\n",
    "                      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(columns = final_colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# final_df = pd.get_dummies(final_colnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the feature matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features_matrix(ast_root):\n",
    "    \"\"\"\n",
    "    Given a concretised & numbered clang ast, returns a matrix of one hot encoded features of node names kind and \n",
    "    whether it's alloc/writeToPointer/sizeOf/other, i.e. our feature matrix\n",
    "    \"\"\"\n",
    "    index = []\n",
    "    kind = {}\n",
    "    spelling = {}\n",
    "    \n",
    "    matrix_df = final_df.copy()\n",
    "\n",
    "    def walk_tree_and_set_properties(node):\n",
    "        out_degree = len(node.children)\n",
    "        in_degree = 1\n",
    "        degree = out_degree + in_degree\n",
    "        \n",
    "        index.append(node.identifier)\n",
    "        \n",
    "        kind[node.identifier] = node.kind\n",
    "        spelling[node.identifier] = node.spelling\n",
    "        \n",
    "        if str(node.spelling) in writeToPointer_list:\n",
    "            spelling[node.identifier] = 'WriteToPointer'\n",
    "        \n",
    "        elif str(node.spelling) in sizeOf_list:\n",
    "            spelling[node.identifier] = 'SizeOf'\n",
    "            \n",
    "        elif str(node.spelling) in alloc_list:\n",
    "            spelling[node.identifier] = 'Alloca'\n",
    "        \n",
    "        else:\n",
    "            spelling[node.identifier] = ''\n",
    "        \n",
    "\n",
    "        for child in node.children:\n",
    "            walk_tree_and_set_properties(child)\n",
    "\n",
    "    walk_tree_and_set_properties(ast_root)\n",
    "    \n",
    "#     return index\n",
    "    \n",
    "    d = {'Identifier': index, 'kind': list(kind.values()), 'spelling': list(spelling.values())}\n",
    "        \n",
    "    ast_df = pd.DataFrame(data = d)\n",
    "    ast_df = ast_df.set_index('Identifier')\n",
    "    \n",
    "    dum_df = pd.get_dummies(ast_df, prefix=['kind', 'spelling'])\n",
    "    \n",
    "    dum_df = dum_df.drop('spelling_', axis=1)\n",
    "    \n",
    "    for col in dum_df.filter(regex='kind_*').columns:\n",
    "        dum_df = dum_df.rename(columns = {col: col.replace('kind_', '')})\n",
    "    \n",
    "    matrix_df['Identifier'] = range(1,len(dum_df)+1)\n",
    "    matrix_df = matrix_df.set_index('Identifier')\n",
    "    matrix_df = matrix_df.fillna(0)\n",
    "    \n",
    "#     df_merge_col = pd.merge(dum_df, matrix_df, on='Identifier')\n",
    "    for i in matrix_df.columns:\n",
    "        if i not in dum_df.columns:\n",
    "            dum_df[i]=0\n",
    "# #     for col in ['SizeOf', 'Alloc', 'WriteToPointer']:\n",
    "# #         if df_merge_col[col].isna().any():\n",
    "# #             df_merge_col[col].fillna(0)\n",
    "            \n",
    "# #     df_merge_col = df_merge_col.dropna(axis='columns')\n",
    "    \n",
    "#     df_merge_col = df_merge_col.set_index('Identifier')\n",
    "    \n",
    "#     return df_merge_col\n",
    "\n",
    "    return dum_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg = generate_features_matrix(ast_roots.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mat = ast_roots.apply(generate_features_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "testcase_ID\n",
       "62550    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62562    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62563    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62564    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62565    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62566    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62567    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62568    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62569    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62570    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62571    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62572    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62573    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62574    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62575    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62576    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62577    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62578    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62579    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62580    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62581    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62582    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62583    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62584    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62585    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62586    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62587    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62588    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62589    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62590    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "                               ...                        \n",
       "62657    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62658    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62659    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62660    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62661    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62662    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62663    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62664    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62665    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62666    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62667    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62668    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62669    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62670    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62671    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62672    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62673    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62674    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62675    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62676    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62677    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62678    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62679    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62680    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62681    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62682    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62683    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62684    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62685    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "62686    [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "Length: 120, dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.iloc[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
