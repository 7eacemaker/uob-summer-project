{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "baseline-model-binary-adj-matrices.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WD7BVc9Bk8jQ"
      },
      "source": [
        "(Uncompleted) Notebook for training and testing the baseline neural network model using adjacency matrices of the AST format of the buffer overflow datapoints. In order for the matrices to be fed into the neural network, they must all be of the same dimensions. We currently pick a subset of the data with small-ish AST matrices (614x614)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K4oesAZzmRhL"
      },
      "source": [
        "# Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W3hzFsJWtCn-",
        "outputId": "42c44efa-32ff-4b5a-c7c6-37742c7b0a1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        }
      },
      "source": [
        "!mkdir -p /project/data && cd /project/data && wget -O adj.pickle https://github.com/dj311/uob-summer-project/raw/master/data/adj.pickle\n",
        "!mkdir -p /project/data && cd /project/data && wget -O buffer_overflow_data.csv.gz https://github.com/dj311/uob-summer-project/raw/master/data/buffer_overflow_data.csv.gz\n",
        "!mkdir -p /project/code\n",
        "%cd /project/code"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-26 12:31:19--  https://github.com/dj311/uob-summer-project/raw/master/data/adj.pickle\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://media.githubusercontent.com/media/dj311/uob-summer-project/master/data/adj.pickle [following]\n",
            "--2019-07-26 12:31:20--  https://media.githubusercontent.com/media/dj311/uob-summer-project/master/data/adj.pickle\n",
            "Resolving media.githubusercontent.com (media.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to media.githubusercontent.com (media.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1320521199 (1.2G) [application/octet-stream]\n",
            "Saving to: ‘adj.pickle’\n",
            "\n",
            "adj.pickle          100%[===================>]   1.23G   247MB/s    in 5.2s    \n",
            "\n",
            "2019-07-26 12:31:25 (243 MB/s) - ‘adj.pickle’ saved [1320521199/1320521199]\n",
            "\n",
            "--2019-07-26 12:31:26--  https://github.com/dj311/uob-summer-project/raw/master/data/buffer_overflow_data.csv.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://media.githubusercontent.com/media/dj311/uob-summer-project/master/data/buffer_overflow_data.csv.gz [following]\n",
            "--2019-07-26 12:31:27--  https://media.githubusercontent.com/media/dj311/uob-summer-project/master/data/buffer_overflow_data.csv.gz\n",
            "Resolving media.githubusercontent.com (media.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to media.githubusercontent.com (media.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3008729 (2.9M) [application/octet-stream]\n",
            "Saving to: ‘buffer_overflow_data.csv.gz’\n",
            "\n",
            "buffer_overflow_dat 100%[===================>]   2.87M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2019-07-26 12:31:27 (35.4 MB/s) - ‘buffer_overflow_data.csv.gz’ saved [3008729/3008729]\n",
            "\n",
            "/project/code\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lO9-8NDKk8ja"
      },
      "source": [
        "# Import & Preprocess Dataset\n",
        "\n",
        "First we import the data from the [previous notebook](./adjacency_matrix.ipynb).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uB5QGgAgk8je",
        "outputId": "a7b38b2c-8ee5-4d58-dcf4-dbd26f2b7555",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import pickle\n",
        "from scipy.sparse import csr_matrix, hstack, vstack\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "run_opts = tf.RunOptions(report_tensor_allocations_upon_oom = True)\n",
        "np.random.seed(1248)\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CGIbzYT0k8jz",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NQwMY_iLk8kC",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('../data/buffer_overflow_data.csv.gz')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "76z9869-k8kQ",
        "colab": {}
      },
      "source": [
        "labels = data.copy()\n",
        "del labels['Unnamed: 0']\n",
        "del labels['Unnamed: 0.1']\n",
        "del labels['filename']\n",
        "del labels['code']\n",
        "del labels['flaw']\n",
        "del labels['flaw_loc']\n",
        "labels = labels.drop_duplicates().sort_values('testcase_ID').reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AyqJFOtWk8kg",
        "colab": {}
      },
      "source": [
        "with open(\"../data/adj.pickle\",'rb') as f:\n",
        "    adj = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PHikMdozk8k1",
        "colab": {}
      },
      "source": [
        "adj = adj.rename(columns={0: 'testcase_ID', 1: 'matrix'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pIdnRsBjk8k-",
        "colab": {}
      },
      "source": [
        "adj_df = pd.merge(labels, adj, on='testcase_ID')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s-qHa46Vk8lK",
        "colab": {}
      },
      "source": [
        "adj_df = adj_df[['testcase_ID', 'matrix', 'bug']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qB06ooxxo--s"
      },
      "source": [
        "Next, find out the maximum size of an adjacency matrix, then convert all matrices to have the same dimension."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HNY1UrHJk8lg",
        "colab": {}
      },
      "source": [
        "adj_df['matrix_size'] = adj_df.matrix.apply(lambda x: x.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p6zGXUxbk8l1",
        "outputId": "f29f32fe-3751-4721-9c65-369a734037ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "adj_df['matrix_size'].describe()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    21502.000000\n",
              "mean      2186.138778\n",
              "std       7239.752920\n",
              "min          4.000000\n",
              "25%        349.000000\n",
              "50%        396.000000\n",
              "75%        614.000000\n",
              "max      44401.000000\n",
              "Name: matrix_size, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VlN1pb2SJW2i"
      },
      "source": [
        "So we know that 75% of the dataset has a matrix size <= 614, which is approximately 18000 datapoints. Picking the full dataset would require matrices of dimension 44401x44401 which require 15 gb of memory each. This is isn't feasible, however picking matrices of size 614x614 or less gives a per matrix size of 3.38mb - far more manageable!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JXR174VPKKzR",
        "outputId": "98f6b25b-e48d-40d4-e54a-61d9a0da8aaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "matrix_size = 614\n",
        "adj_df = adj_df[adj_df['matrix_size'] <= matrix_size]\n",
        "len(adj_df)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16128"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WRYEtD2PhwxN",
        "outputId": "df299881-a565-4aad-b3fe-7894b921e629",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "adj_df['matrix'].values[0]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<395x395 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 788 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MCV9Vwupk8mI",
        "colab": {}
      },
      "source": [
        "def matrix_size_corrector(matrix, size):\n",
        "    '''Pads matrix with zeros to the desired size'''\n",
        "    \n",
        "    rows, columns = matrix.shape[0], matrix.shape[1]\n",
        "    \n",
        "    row_corrector = csr_matrix((size-rows, rows))\n",
        "    col_corrector = csr_matrix((size, size-columns))\n",
        "\n",
        "    matrix = vstack([matrix, row_corrector])\n",
        "    matrix = hstack([matrix, col_corrector])\n",
        "\n",
        "    matrix = matrix.astype(np.int)\n",
        "    \n",
        "    return matrix "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yI1YFNQMk8mp",
        "colab": {}
      },
      "source": [
        "adj_df['matrix'] = adj_df['matrix'].apply(lambda m: matrix_size_corrector(m, matrix_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ga6SW3_qpF80"
      },
      "source": [
        "Now we have a dataframe for each testcase with a sparse representation of its AST in the matrix column, each normalised to matrix_size x matrix_size in size.\n",
        "\n",
        "Now, to input this into our model, we create three numpy arrays, linked by their index:\n",
        "  1. `testcase_ids` gives the testcase ID of each row\n",
        "  2. `adjacency_matrices` is an array of 2D adjacency matrices\n",
        "  3. `labels` tells us whether each row contains a bug or not\n",
        "  \n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oP9x3FKhtSLi",
        "colab": {}
      },
      "source": [
        "testcase_ids = adj_df['testcase_ID'].values\n",
        "adjacency_matrices = adj_df['matrix'].values\n",
        "labels = adj_df['bug'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yXrUMkzYSgUz"
      },
      "source": [
        "Storing all of these matrices in a dense representation at once might cause memory issues. To avoid this, we write a class which generates dense matrices for each of the training batches. \n",
        "\n",
        "We also perform the element wrapping as part of this process (since we can't perform it on the sparse arrays, I think)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dBPBYHNJxm-D",
        "colab": {}
      },
      "source": [
        "class SparseToDenseGenerator(keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self, sparse_matrices, labels, batch_size):\n",
        "        self.sparse_matrices = sparse_matrices\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.sparse_matrices) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, batch_num):\n",
        "        start_index = batch_num * self.batch_size\n",
        "        end_index = (batch_num + 1) * self.batch_size\n",
        "        \n",
        "        batch_sparse = self.sparse_matrices[start_index:end_index]\n",
        "        batch_labels = self.labels[start_index:end_index]\n",
        "        \n",
        "        batch_dense = np.array([sparse_matrix.todense() for sparse_matrix in batch_sparse])\n",
        "        \n",
        "        # TODO: move this somewhere better\n",
        "        # Conv2D requires an extra dimension for \"channels\", so we need to convert our data from\n",
        "        # the shape (batch_size, matrix_rows, matrix_columns)\n",
        "        # to (batch_size, matrix_rows, matrix_columns, 1)\n",
        "        batch_dense = np.reshape(batch_dense, batch_dense.shape + (1, ))\n",
        "\n",
        "        return batch_dense, np.array(batch_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W8rFgat6TFnc"
      },
      "source": [
        "Finally, we generate the train and test splits:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BqJmlJiAk8nE",
        "colab": {}
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(adjacency_matrices, labels, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nda-Xjsjk8ng"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KqlHfs2Kk8ni",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "from keras.layers import Dense, Dropout, Flatten, Reshape, Activation\n",
        "from keras.layers import Conv1D, MaxPooling1D, Conv2D, GlobalMaxPooling2D, MaxPooling2D, Convolution2D\n",
        "from keras.models import Sequential\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.optimizers import RMSprop, Adadelta, Adam\n",
        "from keras.utils import np_utils\n",
        "from keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kmlCQ77P3itE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e54ebbb7-e3ae-4e88-e83c-362de01ab8e2"
      },
      "source": [
        "batch_size = 96\n",
        "epochs = 2\n",
        "num_samples = len(labels)\n",
        "\n",
        "datapoint_shape = (matrix_size, matrix_size, )\n",
        "batch_shape = (batch_size, ) + datapoint_shape\n",
        "\n",
        "steps_per_epoch = int(np.ceil(num_samples/batch_size))\n",
        "\n",
        "kernel_size = (2, 2)\n",
        "strides = max(kernel_size[0] // 3, 1)\n",
        "\n",
        "batch_size, epochs, num_samples, datapoint_shape, batch_shape, steps_per_epoch, kernel_size, strides"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(96, 2, 16128, (614, 614), (96, 614, 614), 168, (2, 2), 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a1aVESBAk8nu",
        "outputId": "9fd0444b-f709-40b9-daac-6501761db2c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        }
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(\n",
        "    data_format='channels_last',\n",
        "    input_shape=(matrix_size, matrix_size, 1),\n",
        "    filters=32,\n",
        "    kernel_size=kernel_size,\n",
        "    strides=strides,\n",
        "))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(kernel_size, padding='same'))\n",
        "\n",
        "model.add(Conv2D(\n",
        "    data_format='channels_last',\n",
        "    input_shape=(matrix_size, matrix_size, 1),\n",
        "    filters=32,\n",
        "    kernel_size=kernel_size,\n",
        "    strides=strides,\n",
        "    activation='relu',\n",
        "))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(kernel_size, padding='same'))\n",
        "\n",
        "model.add(Conv2D(\n",
        "    data_format='channels_last',\n",
        "    input_shape=(matrix_size, matrix_size, 1),\n",
        "    filters=32,\n",
        "    kernel_size=kernel_size,\n",
        "    strides=strides,\n",
        "    activation='relu',\n",
        "))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(kernel_size, padding='same'))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_13 (Conv2D)           (None, 613, 613, 32)      160       \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 613, 613, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 307, 307, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 306, 306, 32)      4128      \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 306, 306, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 153, 153, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 152, 152, 32)      4128      \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 152, 152, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 76, 76, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 184832)            0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 32)                5914656   \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 5,925,217\n",
            "Trainable params: 5,925,217\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gAErQWeyk8nz",
        "outputId": "7cd06c57-cde1-4128-b199-16a607bad6dd",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "training_batch_generator = SparseToDenseGenerator(x_train, y_train, batch_size)\n",
        "\n",
        "model.fit_generator(\n",
        "    generator=training_batch_generator,\n",
        "    epochs=epochs,\n",
        "    steps_per_epoch=steps_per_epoch\n",
        ")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "168/168 [==============================] - 2471s 15s/step - loss: 0.4706 - acc: 0.7488\n",
            "Epoch 2/2\n",
            "168/168 [==============================] - 2474s 15s/step - loss: 0.1354 - acc: 0.9515\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd3c40b4978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNJx-uKp3m7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "with open(\"../data/adjacency-matrix-binary-model.pickle\", \"wb\") as f:\n",
        "    pickle.dump(model, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xnh3rSDNhwx8",
        "colab": {}
      },
      "source": [
        "# Our \"Ideal\" model that we currently can't get to run.\n",
        "\n",
        "# nb_filters = 100\n",
        "# pool_size = 3\n",
        "# input_shape = datapoint_shape\n",
        "\n",
        "# b_model = Sequential()\n",
        "\n",
        "# b_model.add(Convolution2D(nb_filters, (100, 100),\n",
        "#                         padding='same',\n",
        "#                         input_shape=(matrix_size, matrix_size, 1))) # 卷积层1\n",
        "# b_model.add(Activation('sigmoid')) #激活层\n",
        "# b_model.add(Convolution2D(nb_filters, (10, 10))) #卷积层2\n",
        "# b_model.add(Activation('sigmoid')) #激活层\n",
        "# b_model.add(MaxPooling2D(pool_size=pool_size)) #池化层\n",
        "# b_model.add(Dropout(0.25)) #神经元随机失活\n",
        "# b_model.add(Convolution2D(nb_filters, (10, 10))) #卷积层2\n",
        "# b_model.add(Activation('sigmoid')) #激活层\n",
        "# b_model.add(MaxPooling2D(pool_size=pool_size)) #池化层\n",
        "# b_model.add(Dropout(0.25)) #神经元随机失活\n",
        "\n",
        "# b_model.add(Convolution2D(nb_filters, (10, 10))) #卷积层2\n",
        "# b_model.add(Activation('sigmoid')) #激活层\n",
        "# b_model.add(MaxPooling2D(pool_size=pool_size)) #池化层\n",
        "# b_model.add(Dropout(0.25)) #神经元随机失活\n",
        "\n",
        "# #model.add(Flatten()) #拉成一维数据\n",
        "# b_model.add(Dense(128)) #全连接层1\n",
        "# b_model.add(Activation('sigmoid')) #激活层\n",
        "# b_model.add(Dropout(0.5)) #随机失活\n",
        "\n",
        "# # model.add(Flatten()) #拉成一维数据\n",
        "# # model.add(Activation('sigmoid')) #激活层\n",
        "# # model.add(Dropout(0.5)) #随机失活\n",
        "\n",
        "# b_model.add(Dense(1)) #全连接层2\n",
        "# b_model.add(Activation('sigmoid')) #Softmax评分\n",
        "\n",
        "\n",
        "# b_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# b_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w3476EyA3g-V"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2EE3gwh6k8oE",
        "colab": {}
      },
      "source": [
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wk-asGoMk8oK",
        "colab": {}
      },
      "source": [
        "with open('../data/adjacency-matrix-model-binary','wb') as f:\n",
        "    pickle.dump(model,f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nobNI0B1k8oP",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "model.evaluate(x_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vnHI_fjlk8ob",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "from sklearn import metrics\n",
        "import matplotlib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5AJ9ozp5k8og",
        "colab": {}
      },
      "source": [
        "y_predict = model.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gh6M9qH-k8ok",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xrvhYYmFk8oo",
        "colab": {}
      },
      "source": [
        "confusion_matrix = pd.DataFrame(\n",
        "    data=metrics.confusion_matrix(y_test, np.rint(y_predict)),\n",
        ")\n",
        "\n",
        "confusion_figure, confusion_axes = matplotlib.pyplot.subplots()\n",
        "confusion_figure.set_size_inches(15, 12)\n",
        "confusion_axes.set_title(\n",
        "    'Confusion matrix showing the frequency of \\n'\n",
        "    'correct and incorrect bug classification predictions.'\n",
        "    '\\n\\n'  # hack to avoid overlap with x-axis labels below\n",
        ")\n",
        "confusion_axes.xaxis.tick_top()  # move x-axis labels to top of matrix\n",
        "_ = sns.heatmap(\n",
        "    confusion_matrix,\n",
        "    annot=True,\n",
        "    fmt=\"d\",\n",
        "    cmap=sns.color_palette(\"Blues\"),\n",
        "    vmin=0,\n",
        "    ax=confusion_axes,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SFlZwTBIk8o8",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "fpr_keras, tpr_keras, thresholds_keras = roc_curve((y_test.values+0).argmax(axis=1)-5, y_predict.argmax(axis=1)-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a8_7LWSmk8pI",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import auc\n",
        "import matplotlib.pyplot as plt\n",
        "auc_keras = auc(fpr_keras, tpr_keras)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mM9vPPIGk8pO",
        "colab": {}
      },
      "source": [
        "plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIxU3yLeZqk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tn, fp, fn, tp = metrics.confusion_matrix(y_test, np.rint(y_predict)).flatten().tolist()fpr_nn = fp/(fp+tp)\n",
        "fnr_nn = fn/(fn+tn)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}