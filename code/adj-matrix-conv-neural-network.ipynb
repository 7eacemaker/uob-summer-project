{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "baseline-model-binary-adj-matrices.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WD7BVc9Bk8jQ"
      },
      "source": [
        "(Uncompleted) Notebook for training and testing the baseline neural network model using adjacency matrices of the AST format of the buffer overflow datapoints. In order for the matrices to be fed into the neural network, they must all be of the same dimensions. We currently pick a subset of the data with small-ish AST matrices (614x614)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K4oesAZzmRhL"
      },
      "source": [
        "# Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W3hzFsJWtCn-",
        "outputId": "dd3b47e4-83c1-4a7d-b9aa-80ef1882802e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        }
      },
      "source": [
        "!mkdir -p /project/data && cd /project/data && wget -O adj.pickle https://github.com/dj311/uob-summer-project/raw/master/data/adj.pickle\n",
        "!mkdir -p /project/data && cd /project/data && wget -O buffer_overflow_data.csv.gz https://github.com/dj311/uob-summer-project/raw/master/data/buffer_overflow_data.csv.gz\n",
        "!mkdir -p /project/code\n",
        "%cd /project/code"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-26 15:59:33--  https://github.com/dj311/uob-summer-project/raw/master/data/adj.pickle\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://media.githubusercontent.com/media/dj311/uob-summer-project/master/data/adj.pickle [following]\n",
            "--2019-07-26 15:59:33--  https://media.githubusercontent.com/media/dj311/uob-summer-project/master/data/adj.pickle\n",
            "Resolving media.githubusercontent.com (media.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to media.githubusercontent.com (media.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1320521199 (1.2G) [application/octet-stream]\n",
            "Saving to: ‘adj.pickle’\n",
            "\n",
            "adj.pickle          100%[===================>]   1.23G   252MB/s    in 5.0s    \n",
            "\n",
            "2019-07-26 15:59:53 (250 MB/s) - ‘adj.pickle’ saved [1320521199/1320521199]\n",
            "\n",
            "--2019-07-26 15:59:54--  https://github.com/dj311/uob-summer-project/raw/master/data/buffer_overflow_data.csv.gz\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://media.githubusercontent.com/media/dj311/uob-summer-project/master/data/buffer_overflow_data.csv.gz [following]\n",
            "--2019-07-26 15:59:54--  https://media.githubusercontent.com/media/dj311/uob-summer-project/master/data/buffer_overflow_data.csv.gz\n",
            "Resolving media.githubusercontent.com (media.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to media.githubusercontent.com (media.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3008729 (2.9M) [application/octet-stream]\n",
            "Saving to: ‘buffer_overflow_data.csv.gz’\n",
            "\n",
            "buffer_overflow_dat 100%[===================>]   2.87M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2019-07-26 15:59:55 (35.6 MB/s) - ‘buffer_overflow_data.csv.gz’ saved [3008729/3008729]\n",
            "\n",
            "/project/code\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lO9-8NDKk8ja"
      },
      "source": [
        "# Import & Preprocess Dataset\n",
        "\n",
        "First we import the data from the [previous notebook](./adjacency_matrix.ipynb).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uB5QGgAgk8je",
        "outputId": "d21f3f95-1141-4c8f-b83c-ccf66f78892e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import pickle\n",
        "from scipy.sparse import csr_matrix, hstack, vstack\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "run_opts = tf.RunOptions(report_tensor_allocations_upon_oom = True)\n",
        "np.random.seed(1248)\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CGIbzYT0k8jz",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NQwMY_iLk8kC",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('../data/buffer_overflow_data.csv.gz')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "76z9869-k8kQ",
        "colab": {}
      },
      "source": [
        "labels = data.copy()\n",
        "del labels['Unnamed: 0']\n",
        "del labels['Unnamed: 0.1']\n",
        "del labels['filename']\n",
        "del labels['code']\n",
        "del labels['flaw']\n",
        "del labels['flaw_loc']\n",
        "labels = labels.drop_duplicates().sort_values('testcase_ID').reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AyqJFOtWk8kg",
        "colab": {}
      },
      "source": [
        "with open(\"../data/adj.pickle\",'rb') as f:\n",
        "    adj = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PHikMdozk8k1",
        "colab": {}
      },
      "source": [
        "adj = adj.rename(columns={0: 'testcase_ID', 1: 'matrix'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pIdnRsBjk8k-",
        "colab": {}
      },
      "source": [
        "adj_df = pd.merge(labels, adj, on='testcase_ID')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s-qHa46Vk8lK",
        "colab": {}
      },
      "source": [
        "adj_df = adj_df[['testcase_ID', 'matrix', 'bug']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qB06ooxxo--s"
      },
      "source": [
        "Next, find out the maximum size of an adjacency matrix, then convert all matrices to have the same dimension."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HNY1UrHJk8lg",
        "colab": {}
      },
      "source": [
        "adj_df['matrix_size'] = adj_df.matrix.apply(lambda x: x.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p6zGXUxbk8l1",
        "outputId": "87bd5c15-2229-4a91-8729-b165068987f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "adj_df['matrix_size'].describe()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    21502.000000\n",
              "mean      2186.138778\n",
              "std       7239.752920\n",
              "min          4.000000\n",
              "25%        349.000000\n",
              "50%        396.000000\n",
              "75%        614.000000\n",
              "max      44401.000000\n",
              "Name: matrix_size, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VlN1pb2SJW2i"
      },
      "source": [
        "So we know that 75% of the dataset has a matrix size <= 614, which is approximately 18000 datapoints. Picking the full dataset would require matrices of dimension 44401x44401 which require 15 gb of memory each. This is isn't feasible, however picking matrices of size 614x614 or less gives a per matrix size of 3.38mb - far more manageable!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JXR174VPKKzR",
        "outputId": "2ed7c4b3-0357-4348-d95e-3c3ba4a3a03e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "matrix_size = 614\n",
        "adj_df = adj_df[adj_df['matrix_size'] <= matrix_size]\n",
        "len(adj_df)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16128"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WRYEtD2PhwxN",
        "outputId": "b4c62e2e-d443-4064-87ec-fc204b1c219c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "adj_df['matrix'].values[0]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<395x395 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 788 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MCV9Vwupk8mI",
        "colab": {}
      },
      "source": [
        "def matrix_size_corrector(matrix, size):\n",
        "    '''Pads matrix with zeros to the desired size'''\n",
        "    \n",
        "    rows, columns = matrix.shape[0], matrix.shape[1]\n",
        "    \n",
        "    row_corrector = csr_matrix((size-rows, rows))\n",
        "    col_corrector = csr_matrix((size, size-columns))\n",
        "\n",
        "    matrix = vstack([matrix, row_corrector])\n",
        "    matrix = hstack([matrix, col_corrector])\n",
        "\n",
        "    matrix = matrix.astype(np.int)\n",
        "    \n",
        "    return matrix "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yI1YFNQMk8mp",
        "colab": {}
      },
      "source": [
        "adj_df['matrix'] = adj_df['matrix'].apply(lambda m: matrix_size_corrector(m, matrix_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ga6SW3_qpF80"
      },
      "source": [
        "Now we have a dataframe for each testcase with a sparse representation of its AST in the matrix column, each normalised to matrix_size x matrix_size in size.\n",
        "\n",
        "Now, to input this into our model, we create three numpy arrays, linked by their index:\n",
        "  1. `testcase_ids` gives the testcase ID of each row\n",
        "  2. `adjacency_matrices` is an array of 2D adjacency matrices\n",
        "  3. `labels` tells us whether each row contains a bug or not\n",
        "  \n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oP9x3FKhtSLi",
        "colab": {}
      },
      "source": [
        "testcase_ids = adj_df['testcase_ID'].values\n",
        "adjacency_matrices = adj_df['matrix'].values\n",
        "labels = adj_df['bug'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yXrUMkzYSgUz"
      },
      "source": [
        "Storing all of these matrices in a dense representation at once might cause memory issues. To avoid this, we write a class which generates dense matrices for each of the training batches. \n",
        "\n",
        "We also perform the element wrapping as part of this process (since we can't perform it on the sparse arrays, I think)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dBPBYHNJxm-D",
        "colab": {}
      },
      "source": [
        "class SparseToDenseGenerator(keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self, sparse_matrices, labels, batch_size):\n",
        "        self.sparse_matrices = sparse_matrices\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.sparse_matrices) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, batch_num):\n",
        "        start_index = batch_num * self.batch_size\n",
        "        end_index = (batch_num + 1) * self.batch_size\n",
        "        \n",
        "        batch_sparse = self.sparse_matrices[start_index:end_index]\n",
        "        batch_labels = self.labels[start_index:end_index]\n",
        "        \n",
        "        batch_dense = np.array([sparse_matrix.todense() for sparse_matrix in batch_sparse])\n",
        "        \n",
        "        # TODO: move this somewhere better\n",
        "        # Conv2D requires an extra dimension for \"channels\", so we need to convert our data from\n",
        "        # the shape (batch_size, matrix_rows, matrix_columns)\n",
        "        # to (batch_size, matrix_rows, matrix_columns, 1)\n",
        "        batch_dense = np.reshape(batch_dense, batch_dense.shape + (1, ))\n",
        "\n",
        "        return batch_dense, np.array(batch_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W8rFgat6TFnc"
      },
      "source": [
        "Finally, we generate the train and test splits:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BqJmlJiAk8nE",
        "colab": {}
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(adjacency_matrices, labels, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nda-Xjsjk8ng"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KqlHfs2Kk8ni",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "from keras.layers import Dense, Dropout, Flatten, Reshape, Activation\n",
        "from keras.layers import Conv1D, MaxPooling1D, Conv2D, GlobalMaxPooling2D, MaxPooling2D, Convolution2D\n",
        "from keras.models import Sequential\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.optimizers import RMSprop, Adadelta, Adam\n",
        "from keras.utils import np_utils\n",
        "from keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kmlCQ77P3itE",
        "outputId": "5c5f0c07-d34d-4f10-f514-3aab18d0fca4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "batch_size = 96\n",
        "epochs = 5\n",
        "num_samples = len(labels)\n",
        "\n",
        "datapoint_shape = (matrix_size, matrix_size, )\n",
        "batch_shape = (batch_size, ) + datapoint_shape\n",
        "\n",
        "steps_per_epoch = int(np.ceil(num_samples/batch_size))\n",
        "\n",
        "kernel_size = (2, 2)\n",
        "strides = max(kernel_size[0] // 3, 1)\n",
        "\n",
        "batch_size, epochs, num_samples, datapoint_shape, batch_shape, steps_per_epoch, kernel_size, strides"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(96, 5, 16128, (614, 614), (96, 614, 614), 168, (2, 2), 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a1aVESBAk8nu",
        "outputId": "4d4e5f24-ac9c-4c46-eac0-9f1f5e9db26a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(\n",
        "    data_format='channels_last',\n",
        "    input_shape=(matrix_size, matrix_size, 1),\n",
        "    filters=32,\n",
        "    kernel_size=kernel_size,\n",
        "    strides=strides,\n",
        "))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(kernel_size, padding='same'))\n",
        "\n",
        "model.add(Conv2D(\n",
        "    data_format='channels_last',\n",
        "    input_shape=(matrix_size, matrix_size, 1),\n",
        "    filters=32,\n",
        "    kernel_size=kernel_size,\n",
        "    strides=strides,\n",
        "    activation='relu',\n",
        "))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(kernel_size, padding='same'))\n",
        "\n",
        "model.add(Conv2D(\n",
        "    data_format='channels_last',\n",
        "    input_shape=(matrix_size, matrix_size, 1),\n",
        "    filters=32,\n",
        "    kernel_size=kernel_size,\n",
        "    strides=strides,\n",
        "    activation='relu',\n",
        "))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(kernel_size, padding='same'))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0726 16:00:17.741810 140183665493888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0726 16:00:17.757972 140183665493888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0726 16:00:17.761829 140183665493888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0726 16:00:17.780237 140183665493888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0726 16:00:17.867340 140183665493888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0726 16:00:17.878172 140183665493888 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0726 16:00:17.928620 140183665493888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0726 16:00:17.935703 140183665493888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0726 16:00:17.941003 140183665493888 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 613, 613, 32)      160       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 613, 613, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 307, 307, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 306, 306, 32)      4128      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 306, 306, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 153, 153, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 152, 152, 32)      4128      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 152, 152, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 76, 76, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 184832)            0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                5914656   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 5,925,217\n",
            "Trainable params: 5,925,217\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gAErQWeyk8nz",
        "outputId": "500cfefe-b4d0-403f-ee45-8f4609852f8c",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "training_batch_generator = SparseToDenseGenerator(x_train, y_train, batch_size)\n",
        "\n",
        "model.fit_generator(\n",
        "    generator=training_batch_generator,\n",
        "    epochs=epochs,\n",
        "    steps_per_epoch=steps_per_epoch\n",
        ")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "168/168 [==============================] - 2402s 14s/step - loss: 0.5190 - acc: 0.7042\n",
            "Epoch 2/5\n",
            "168/168 [==============================] - 2368s 14s/step - loss: 0.1684 - acc: 0.9265\n",
            "Epoch 3/5\n",
            "168/168 [==============================] - 2449s 15s/step - loss: 0.0734 - acc: 0.9797\n",
            "Epoch 4/5\n",
            "168/168 [==============================] - 2473s 15s/step - loss: 0.0348 - acc: 0.9912\n",
            "Epoch 5/5\n",
            "168/168 [==============================] - 2472s 15s/step - loss: 0.0201 - acc: 0.9944\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7ec7129d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xnh3rSDNhwx8",
        "colab": {}
      },
      "source": [
        "# Our \"Ideal\" model that we currently can't get to run.\n",
        "\n",
        "# nb_filters = 100\n",
        "# pool_size = 3\n",
        "# input_shape = datapoint_shape\n",
        "\n",
        "# b_model = Sequential()\n",
        "\n",
        "# b_model.add(Convolution2D(nb_filters, (100, 100),\n",
        "#                         padding='same',\n",
        "#                         input_shape=(matrix_size, matrix_size, 1))) # 卷积层1\n",
        "# b_model.add(Activation('sigmoid')) #激活层\n",
        "# b_model.add(Convolution2D(nb_filters, (10, 10))) #卷积层2\n",
        "# b_model.add(Activation('sigmoid')) #激活层\n",
        "# b_model.add(MaxPooling2D(pool_size=pool_size)) #池化层\n",
        "# b_model.add(Dropout(0.25)) #神经元随机失活\n",
        "# b_model.add(Convolution2D(nb_filters, (10, 10))) #卷积层2\n",
        "# b_model.add(Activation('sigmoid')) #激活层\n",
        "# b_model.add(MaxPooling2D(pool_size=pool_size)) #池化层\n",
        "# b_model.add(Dropout(0.25)) #神经元随机失活\n",
        "\n",
        "# b_model.add(Convolution2D(nb_filters, (10, 10))) #卷积层2\n",
        "# b_model.add(Activation('sigmoid')) #激活层\n",
        "# b_model.add(MaxPooling2D(pool_size=pool_size)) #池化层\n",
        "# b_model.add(Dropout(0.25)) #神经元随机失活\n",
        "\n",
        "# #model.add(Flatten()) #拉成一维数据\n",
        "# b_model.add(Dense(128)) #全连接层1\n",
        "# b_model.add(Activation('sigmoid')) #激活层\n",
        "# b_model.add(Dropout(0.5)) #随机失活\n",
        "\n",
        "# # model.add(Flatten()) #拉成一维数据\n",
        "# # model.add(Activation('sigmoid')) #激活层\n",
        "# # model.add(Dropout(0.5)) #随机失活\n",
        "\n",
        "# b_model.add(Dense(1)) #全连接层2\n",
        "# b_model.add(Activation('sigmoid')) #Softmax评分\n",
        "\n",
        "\n",
        "# b_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# b_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w3476EyA3g-V"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2EE3gwh6k8oE",
        "colab": {}
      },
      "source": [
        " import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wk-asGoMk8oK",
        "colab": {}
      },
      "source": [
        "with open('../data/adjacency-matrix-model-binary','wb') as f:\n",
        "    pickle.dump(model,f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nobNI0B1k8oP",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e81b6358-f9aa-4ee2-eaf9-8b65fd33cac9"
      },
      "source": [
        "test_batch_generator = SparseToDenseGenerator(x_test, y_test, batch_size)\n",
        "model.evaluate_generator(\n",
        "    generator=test_batch_generator)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.021008730195670563, 0.9944203250983156]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vnHI_fjlk8ob",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "from sklearn import metrics\n",
        "import matplotlib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5AJ9ozp5k8og",
        "colab": {}
      },
      "source": [
        "y_predict = model.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gh6M9qH-k8ok",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xrvhYYmFk8oo",
        "colab": {}
      },
      "source": [
        "confusion_matrix = pd.DataFrame(\n",
        "    data=metrics.confusion_matrix(y_test, np.rint(y_predict)),\n",
        ")\n",
        "\n",
        "confusion_figure, confusion_axes = matplotlib.pyplot.subplots()\n",
        "confusion_figure.set_size_inches(15, 12)\n",
        "confusion_axes.set_title(\n",
        "    'Confusion matrix showing the frequency of \\n'\n",
        "    'correct and incorrect bug classification predictions.'\n",
        "    '\\n\\n'  # hack to avoid overlap with x-axis labels below\n",
        ")\n",
        "confusion_axes.xaxis.tick_top()  # move x-axis labels to top of matrix\n",
        "_ = sns.heatmap(\n",
        "    confusion_matrix,\n",
        "    annot=True,\n",
        "    fmt=\"d\",\n",
        "    cmap=sns.color_palette(\"Blues\"),\n",
        "    vmin=0,\n",
        "    ax=confusion_axes,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SFlZwTBIk8o8",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "fpr_keras, tpr_keras, thresholds_keras = roc_curve((y_test.values+0).argmax(axis=1)-5, y_predict.argmax(axis=1)-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a8_7LWSmk8pI",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import auc\n",
        "import matplotlib.pyplot as plt\n",
        "auc_keras = auc(fpr_keras, tpr_keras)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mM9vPPIGk8pO",
        "colab": {}
      },
      "source": [
        "plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIxU3yLeZqk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tn, fp, fn, tp = metrics.confusion_matrix(y_test, np.rint(y_predict)).flatten().tolist()fpr_nn = fp/(fp+tp)\n",
        "fnr_nn = fn/(fn+tn)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}