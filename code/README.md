All of our files have a tag at the beginning, relating to a section of our project:

  - DG: Data Generation - code used to generate and process the data used in our models
  - ILP: Inductive Logic Programming - code used to generate ILP rules and background knowledge and code used to run ILP systems and generate a rule from this
  - ML: Machine Learning - code used to create any of our other machine learning models (stacking model and neural networks)
  
Three notebooks must be ran on Google Colab in order to have enough RAM. These are:
  
  - ML-adj-matrix-conv-neural-network.ipynb
  - ML-adj-feat-matrix-conv-neural-network.ipynb
  - ML-evaluating-final-models.ipynb
  
Some notebooks have a "Colab Setup" section. These setup the Google colab environment with the correct source files and data. They don't need to (and shouldn't) be run on your own machine.

# Data Generation

We started with two datasets containing labelled examples of vulnerabilities in C/C++:
  1. [Juliet Software Assurance Dataset](https://samate.nist.gov/SARD/testsuite.php) :: The initial processing of this dataset was done in the ["Exploring Juliet"](./DG-exploring-juliet.ipynb) notebook.
  2. [Draper VDISC Dataset](https://osf.io/d45bw/) :: The initial processing of this dataset was done in the ["Exploring VDISC"](./DG-exploring-vdisc.ipynb) notebook.
  
The result of these two notebooks is a Pandas dataframe for each dataset, each normalised to a simliar structure. The resultant files are [../data/juliet_split.csv.gz](../data/juliet_split.csv.gz), ['data/vdisc_train.csv.gz'](data/vdisc_train.csv.gz), ['data/vdisc_test.csv.gz'](data/vdisc_test.csv.gz), and ['data/vdisc_validate.csv.gz'](data/vdisc_validate.csv.gz).

We then focused on buffer overflow examples in the Juliet dataset. This subset is generated by the [./DG-bug-picking.ipynb](./DG-bug-picking.ipynb) and saved to [../data/buffer_overflow_data.csv.gz](../data/buffer_overflow_data.csv.gz).

From here onwards, the data processing is split into:
  1. [./preprocess_code.py](./preprocess_code.py) prepares the data for our machine learning models. It uses clang to generate an abstract syntax tree for each datapoint. It then generates graph (graph2vec) and node (node2vec) embeddings.
  2. [./DG-generating-adjacency-feature-matrix.ipynb](./DG-generating-adjacency-feature-matrix.ipynb) prepares the data for the machine learning models which use the adjacency and feature matrix representations.
  2. [./DG-generate-minimal-ilp-dataset.ipynb](./DG-generate-minimal-ilp-dataset.ipynb]) prepares the data for ILP using  [Joern](https://joern.io/) code property graphs. This uses the [`joern-cfg-to-prolog.scala`](joern-cfg-to-prolog.scala) script to convert our code property graph into a set of Prolog facts.



# ILP

We started by handcrafting rules and backrgound knowledge for a small set of examples. This work was done using Prolo and the Metagol ILP system. The result of this work can be found in the [`LP-handcrafted-ilp-rules-for-metagol.pl`](ILP-handcrafted-ilp-rules-for-metagol.pl). 

Using the Prolog representation of our ILP dataset generate by [./DG-generate-minimal-ilp-dataset.ipynb](./DG-generate-minimal-ilp-dataset.ipynb]) we then generate Progol scripts using a variety of different settings and representations, whilst analsysing their effectiveness:

  1. [ILP-joern-ey-into-progol.ipynb](ILP-joern-ey-into-progol.ipynb)
  2. [ILP-joern-ey-into-progol-tree-tag.ipynb](ILP-joern-ey-into-progol-tree-tag.ipynb)
  3. [ILP-progol-tag-alloc-and-write-nodes.ipynb](ILP-progol-tag-alloc-and-write-nodes.ipynb)
  3. [ILP-progol-tag-alloc-and-write-nodes-force.ipynb](ILP-progol-tag-alloc-and-write-nodes-force.ipynb)
  
We use the `graph_visualisation.py` script to visualise the resulting rules output by Progol (and Aleph) from the above notebooks (this is turn uses the `ILP-joern_cfg_to_dot.scala` script).
 
 We performed further investigation into ILP systems using the Aleph ILP system. This work was done in the [ILP-joern-ey-into-aleph.ipynb](ILP-joern-ey-into-aleph.ipynb) notebook.
 
 During this time, we took a reverse-engineering approach to find an ideal Progol rule. This allowed us to ensure our background knowledge was sufficiently expressive. This work was done in the [ILP-checking-an-ideal-rule-in-prolog.ipynb](ILP-checking-an-ideal-rule-in-prolog.ipynb).
 
# ML

  - ML-dense-neural-network-graph2vec.ipynb :: construction of a baseline feed-foward neural network using the graph2vec embedding generated via preprocess_code.py.
  - ML-dense-neural-network-graph2vec.ipynb :: construction of convolutional neural network using the graph2vec embedding generated via preprocess_code.py.
  - ML-ml-model-comparison-and-stacking-binary.ipynb :: construction of our stacking models.
  - ML-adj-matrix-conv-neural-network.ipynb :: construction of convolutional neural network using the adjacency matrix representation (standard and random padding) generated via DG-generating-adjacency-feature-matrix.ipynb.
  - ML-adj-feat-matrix-conv-neural-network.ipynb :: construction of convolutional neural network using the adjacency and feature matrices representation (standard and random padding) generated via DG-generating-adjacency-feature-matrix.ipynb.
  - ML-adj-matrix-visualisation.ipynb :: visualising the adjacency matrix representation of source code.
  - ML-evaluating-final-models.ipynb :: evaluation of machine learning models.
  
# ML Periphery Experiments

  - ML-old-baseline-model-comparison_all_data.ipynb, ML-old-baseline-model-final-all-data.ipynb :: construction of dense feed forward neural network on graph2vec embeddings of the entire Juliet dataset involving multiple bug types.
  - ML-visualisation-comparing-model-predictions.m :: t-SNE embeddings of machinel learning models' predictions.
  - ML-node2vec-naive-model.ipynb :: construction of dense feed forward neural network on node2vec embeddings.

[Uncompleted] In the following notebooks, we experimented with out of sample performance with the VDISC dataset:

 - ML-outsample-vdisc-comparison-and-stacking-binary.ipynb
 - ML-outsample-vdisc-conv-neural-network-graph2vec.ipynb
 - ML-outsample-vdisc-dense-neural-network-graph2vec.ipynb
 - ML_adj_matrix_conv_neural_network_vdisc.ipynb

Move to other documentation files:
 
 
## Guide of preprocess_code
Most of our codes for preprecessing are in this file `preprocess_code.py`


### The usage of clang
In our project, we need to get ast tree for every C/C++ source code. This package `clang.cindex` provides us a way to generate `ast_root` from which we can walk all the children nodes in this ast tree.

### How to use the tree
When we get the tree, we want to number each tree in order to get an edgelist or something. 

But every time we run `get_children` for our nodes, it returns a new object which will lose information. So you can use `concretise_ast` to solve this problem. 

After that, we can number the node by using `number_ast_nodes`(start with `counter = 1`). It is easy to generate edgelist by using `generate_edgelist` funtion which walk the graph and generate the edgelist from `ast_root` generated by `clang` before.

### Graph2vec

### Dask
Since the data size is large, we use dask in our function named `preprocess_all_for_graph2vec` to do it seperately.

## Node2vec
Node2vec requires edgelist for each ast. So by simply removing the features we get node2vec function named `process_for_node2vec` which can be used in `preprocess_all_for_adjmatrix` to get adj matrix and `preprocess_all_for_node2vec`.

### Edgelist
The outputs of all the edgelists are saved into other git repo named [`uob-summer-project-node2vec`](https://github.com/xihajun/uob-summer-project-node2vec)

## Adjacacency and Feature Matrices
The adjacency matrix is saved into the file `../data/adj.pickle`.
Our adjencency matrices for some reason is undirected, to fix this issue, we just need to get rid of the up-triangle values.
The feature matrix is saved into the file `../data/feature_matrix.pickle`.
Our feature matrices have columns that represent properties of a node and each node is one-hot encoded (with each row representing a node).


...continue
