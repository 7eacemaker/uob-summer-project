All of our files have a tag at the beginning, relating to a section of our project:

  - DG: Data Generation - code used to generate and process the data used in our models
  - ILP: Inductive Logic Programming - code used to generate ILP rules and background knowledge and code used to run ILP systems and generate a rule from this
  - ML: Machine Learning - code used to create any of our other machine learning models (stacking model and neural networks)
  
Three notebooks must be ran on Google Colab in order to have enough RAM. These are:
  
  - ML-adj-matrix-conv-neural-network.ipynb
  - ML-adj-feat-matrix-conv-neural-network.ipynb
  - ML-evaluating-final-models.ipynb
  
Some notebooks have a "Colab Setup" section. These setup the Google colab environment with the correct source files and data. They don't need to (and shouldn't) be run on your own machine.

# Data Generation

We started with two datasets containing labelled examples of vulnerabilities in C/C++:
  1. [Juliet Software Assurance Dataset](https://samate.nist.gov/SARD/testsuite.php) :: The initial processing of this dataset was done in the ["Exploring Juliet"](./DG-exploring-juliet.ipynb) notebook.
  2. [Draper VDISC Dataset](https://osf.io/d45bw/) :: The initial processing of this dataset was done in the ["Exploring VDISC"](./DG-exploring-vdisc.ipynb) notebook.
  
The result of these two notebooks is a Pandas dataframe for each dataset, each normalised to a simliar structure. The resultant files are [../data/juliet_split.csv.gz](../data/juliet_split.csv.gz), ['data/vdisc_train.csv.gz'](data/vdisc_train.csv.gz), ['data/vdisc_test.csv.gz'](data/vdisc_test.csv.gz), and ['data/vdisc_validate.csv.gz'](data/vdisc_validate.csv.gz).

We then focused on buffer overflow examples in the Juliet dataset. This subset is generated by the [./DG-bug-picking.ipynb](./DG-bug-picking.ipynb) and saved to [../data/buffer_overflow_data.csv.gz](../data/buffer_overflow_data.csv.gz).

From here onwards, the data processing is split into:
  1. [./preprocess_code.py](./preprocess_code.py) prepares the data for our machine learning models. It uses clang to generate an abstract syntax tree for each datapoint. It then generates graph (graph2vec) and node (node2vec) embeddings.
  2. [./DG-generating-adjacency-feature-matrix.ipynb](./DG-generating-adjacency-feature-matrix.ipynb) prepares the data for the machine learning models which use the adjacency and feature matrix representations.
  2. [./DG-generate-minimal-ilp-dataset.ipynb](./DG-generate-minimal-ilp-dataset.ipynb]) prepares the data for ILP using  [Joern](https://joern.io/) code property graphs. This uses the [`joern-cfg-to-prolog.scala`](joern-cfg-to-prolog.scala) script to convert our code property graph into a set of Prolog facts.



# ILP

We started by handcrafting rules and backrgound knowledge for a small set of examples. This work was done using Prolo and the Metagol ILP system. The result of this work can be found in the [`LP-handcrafted-ilp-rules-for-metagol.pl`](ILP-handcrafted-ilp-rules-for-metagol.pl). 

Using the Prolog representation of our ILP dataset generate by [./DG-generate-minimal-ilp-dataset.ipynb](./DG-generate-minimal-ilp-dataset.ipynb]) we then generate Progol scripts using a variety of different settings and representations, whilst analsysing their effectiveness:

  1. [ILP-joern-ey-into-progol.ipynb](ILP-joern-ey-into-progol.ipynb)
  2. [ILP-joern-ey-into-progol-tree-tag.ipynb](ILP-joern-ey-into-progol-tree-tag.ipynb)
  3. [ILP-progol-tag-alloc-and-write-nodes.ipynb](ILP-progol-tag-alloc-and-write-nodes.ipynb)
  3. [ILP-progol-tag-alloc-and-write-nodes-force.ipynb](ILP-progol-tag-alloc-and-write-nodes-force.ipynb)
  
We use the `graph_visualisation.py` script to visualise the resulting rules output by Progol (and Aleph) from the above notebooks (this is turn uses the `ILP-joern_cfg_to_dot.scala` script).
 
 We performed further investigation into ILP systems using the Aleph ILP system. This work was done in the [ILP-joern-ey-into-aleph.ipynb](ILP-joern-ey-into-aleph.ipynb) notebook.
 
 During this time, we took a reverse-engineering approach to find an ideal Progol rule. This allowed us to ensure our background knowledge was sufficiently expressive. This work was done in the [ILP-checking-an-ideal-rule-in-prolog.ipynb](ILP-checking-an-ideal-rule-in-prolog.ipynb).
 
 
 
Move to other documentation files:
 
 
## Guide of preprocess_code
Most of our codes for preprecessing are in this file `preprocess_code.py`


### The usage of clang
In our project, we need to get ast tree for every C/C++ source code. This package `clang.cindex` provides us a way to generate `ast_root` from which we can walk all the children nodes in this ast tree.

### How to use the tree
When we get the tree, we want to number each tree in order to get an edgelist or something. 

But every time we run `get_children` for our nodes, it returns a new object which will lose information. So you can use `concretise_ast` to solve this problem. 

After that, we can number the node by using `number_ast_nodes`(start with `counter = 1`). It is easy to generate edgelist by using `generate_edgelist` funtion which walk the graph and generate the edgelist from `ast_root` generated by `clang` before.

### Graph2vec

### Dask
Since the data size is large, we use dask in our function named `preprocess_all_for_graph2vec` to do it seperately.

## Node2vec
Node2vec requires edgelist for each ast. So by simply removing the features we get node2vec function named `process_for_node2vec` which can be used in `preprocess_all_for_adjmatrix` to get adj matrix and `preprocess_all_for_node2vec`.

### Edgelist
The outputs of all the edgelists are saved into other git repo named [`uob-summer-project-node2vec`](https://github.com/xihajun/uob-summer-project-node2vec)

## Adjacacency and Feature Matrices
The adjacency matrix is saved into the file `../data/adj.pickle`.
Our adjencency matrices for some reason is undirected, to fix this issue, we just need to get rid of the up-triangle values.
The feature matrix is saved into the file `../data/feature_matrix.pickle`.
Our feature matrices have columns that represent properties of a node and each node is one-hot encoded (with each row representing a node).


...continue


## Progol

Progol outputs lines like:
```
f=393,p=400,n=0,h=0
```
when searching. But what do they mean? We trauled through the Progol source code to provide you with this valuable information:

```
/* r_search 
  A*-like algorithm based on pseudo-compression
  T is background knowledge.
  C = h<-B is most specific i-bounded clause in mode language s.t. T/\C |= e.
  Search space is C' = h<-B' where B' subset of B. Open list is sorted
  in descending order on heuristic function
  
        g = (c+n)-p             [neg. pseudo-compression]
          = (c+negcover(C'))-poscover(C')       [(c+n)-p]
  
        where cardinality c = |B'|
        
  Let bestclosed = <g0,p0,c0>
  Let Open = <<g1,p1,c1>,..<gn,pn,cn>>. Search terminates when
  n0=0 and p0>pi for 2<=i<=p. Proof that search returns consistent clause
  with maximum positive coverage is immediate from the fact that
  p1>pi and p decreases monotonically along any path in the search.
  When there is more than one clause with this coverage, the one with
  maximum compression will be returned.
  
  Search pruned for
        a) Children of node i with ni=0
        b) Children of node with pi<ci  [there can be no eventual compression]
        c) Any node i with gi<g0 when n0=0
        d) Any node with ci>=Clause-length limit

  Lower-bound distance to goal h can be introduced by tabulating
        the "distance", minimal number of atoms from
        each atom to an atom which computes the output variables
        in the bottom clause. This number is minimised over a given clause
        to give h, the number of required atoms to compute output variables.
        When no outputs in the head then distance is zero for
        all atoms in bottom. Atoms not on path to output
        variables are given a large non-negative distance value.
  
  Actual state record used is the following integer array
  
        f - g+h = ((c+n)-p)+h
        p - positive coverage (n calcuated as f+p-(c+h))
        c - the clause length
        h - min. no atoms to output
        a/u - encoded binding of chosen atom
        par - parent state
  
 */
```
