All of our files have a tag at the beginning, relating to a section of our project:

  - DG: Data Generation - code used to generate and process the data used in our models
  - ILP: Inductive Logic Programming - code used to generate ILP rules and background knowledge and code used to run ILP systems and generate a rule from this
  - ML: Machine Learning - code used to create any of our other machine learning models (stacking model and neural networks)

# Data Generation

We started with two datasets containing labelled examples of vulnerabilities in C/C++:
  1. [Juliet Software Assurance Dataset](https://samate.nist.gov/SARD/testsuite.php) :: The initial processing of this dataset was done in the ["Exploring Juliet"](./DG-exploring-juliet.ipynb) notebook.
  2. [Draper VDISC Dataset](https://osf.io/d45bw/) :: The initial processing of this dataset was done in the ["Exploring VDISC"](./DG-exploring-vdisc.ipynb) notebook.
  
The result of these two notebooks is a Pandas dataframe for each dataset, each normalised to a simliar structure. These resultant files are [../data/juliet_split.csv.gz](../data/juliet_split.csv.gz), ['data/vdisc_train.csv.gz'](data/vdisc_train.csv.gz), ['data/vdisc_test.csv.gz'](data/vdisc_test.csv.gz), and ['data/vdisc_validate.csv.gz'](data/vdisc_validate.csv.gz).

We then focused on buffer overflow examples in the Juliet dataset. This subset is generated by the [./DG-bug-picking.ipynb](./DG-bug-picking.ipynb) and saved to [../data/buffer_overflow_data.csv.gz](../data/buffer_overflow_data.csv.gz).

From here onwards, the data processing is split into:
  1. [./preprocess_code.py](./preprocess_code.py) prepares the data for our machine learning models. It uses clang to generate an abstract syntax tree for each datapoint, then generates graph (graph2vec) and node (node2vec) embeddings.
  2. [./DG-generating-adjacency-feature-matrix.ipynb](./DG-generating-adjacency-feature-matrix.ipynb) prepares the data for our machine learning models using graph/node embeddings.
  2. [./DG-generate-minimal-ilp-dataset.ipynb](./DG-generate-minimal-ilp-dataset.ipynb])prepares the data for ILP using [Joern](https://joern.io/) code property graphs.


## Guide of preprocess_code
Most of our codes for preprecessing are in this file `preprocess_code.py`


### The usage of clang
In our project, we need to get ast tree for every C/C++ source code. This package `clang.cindex` provides us a way to generate `ast_root` from which we can walk all the children nodes in this ast tree.

### How to use the tree
When we get the tree, we want to number each tree in order to get an edgelist or something. 

But every time we run `get_children` for our nodes, it returns a new object which will lose information. So you can use `concretise_ast` to solve this problem. 

After that, we can number the node by using `number_ast_nodes`(start with `counter = 1`). It is easy to generate edgelist by using `generate_edgelist` funtion which walk the graph and generate the edgelist from `ast_root` generated by `clanng` before.

### Graph2vec

### Dask
Since the data size is large, we use dask in our function named `preprocess_all_for_graph2vec` to do it seperately.

## Node2vec
Node2vec requires edgelist for each ast. So by simplily removing the features we get node2vec function named `process_for_node2vec` which can be used in `preprocess_all_for_adjmatrix` to get adj matrix and `preprocess_all_for_node2vec`.

### Edgelist
The outputs of all the edgelists are saved into other git repo named [`uob-summer-project-node2vec`](https://github.com/xihajun/uob-summer-project-node2vec).Edglist

## Adjacacency and Feature Matrices
The adjencency matrix is saved into the file `../data/adj.pickle`.
Our adjencency matrices for some reason is undirected, to fix this issue, we just need to get rid of the up-triangle values.
The feature matrix is saved into the file `../data/feature_matrix.pickle`.
Our feature matrices have columns that represent properties of a node and each node is one-hot encoded (with each row representing a node).


...continue

